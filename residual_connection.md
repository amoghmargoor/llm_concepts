# Understanding Residual Connections and Vanishing Gradient: From Basics to DeepSeek's Manifold-Constrained Hyper-Connections

*A beginner-friendly guide to one of deep learning's basic concept that relates to recent DeepSeek's paper: https://arxiv.org/abs/2512.24880. Note the paper is only briefly discussed towards the end and not really for audience that knows the basics of Residual connections and Gradients. This blog is also fully recreated by LLM (Claude) and only for beginners with probably no background in deep learning.*

---

## Table of Contents
1. [Residual Connections: The Foundation](#residual-connections)
2. [Understanding Gradients: How Neural Networks Learn](#gradients)
3. [The DeepSeek Innovation: Manifold-Constrained Hyper-Connections](#deepseek)

---

## Part 1: Residual Connections - The Foundation {#residual-connections}

### The Problem That Changed Everything

Imagine you're trying to build the world's tallest LEGO tower. Intuitively, you'd think that more blocks would make a better, more capable tower. But in the world of neural networks before 2016, something strange happened: **deeper networks often performed worse than shallow ones.**

This wasn't just counterintuitiveâ€”it was a fundamental barrier to progress in artificial intelligence.

### Traditional Neural Networks (Without Residual Connections)

First, let's understand what we had before residual connections.

In a traditional neural network, information flows through layers sequentially:

```
xâ‚ = F(xâ‚€, Wâ‚€)
xâ‚‚ = F(xâ‚, Wâ‚)
xâ‚ƒ = F(xâ‚‚, Wâ‚‚)
...
```

Where:
- **xâ‚€** is your input (like an image)
- **F** is some transformation (convolution, matrix multiplication, etc.)
- **W** represents the learnable parameters (weights)
- Each layer completely transforms the input from the previous layer

**The Problem:** When you stack many layers (say 50 or 100), two bad things happen:
1. **Vanishing gradients**: The learning signal gets weaker and weaker as it flows backward during training
2. **Degradation**: Surprisingly, deeper networks sometimes perform worse than shallow ones

### What Are Residual Connections?

Residual connections (or "skip connections") were introduced in 2016 with ResNets, and they revolutionized how we build neural networks. The core idea is elegantly simple: instead of forcing each layer to learn the complete transformation of its input, let it learn just the **residual** (the difference or adjustment).

#### The Mathematics

For a single layer with a residual connection:

```
x_{l+1} = x_l + F(x_l, W_l)
```

Let me break this down:
- **x_l**: The input to layer l (your "current state")
- **F(x_l, W_l)**: The residual functionâ€”what the layer learns to add or adjust
- **x_{l+1}**: The output, which is the input PLUS the adjustment
- The **"+ x_l"** part is the skip connection (identity mapping)

### The Power of Stacking Layers

When you stack multiple residual layers, something beautiful happens. Let's trace the flow from layer l to layer L (several layers deeper):

```
x_{l+1} = x_l + F(x_l, W_l)
x_{l+2} = x_{l+1} + F(x_{l+1}, W_{l+1})
x_{l+3} = x_{l+2} + F(x_{l+2}, W_{l+2})
...
```

If we expand this recursively, we get:

```
x_L = x_l + Î£ F(x_i, W_i)    [sum from i=l to L-1]
```

**Translation:** The output at layer L equals the input at layer l, plus the sum of all the residual adjustments in between.

```
Final Output = Original Input + All Adjustments
```

### A Concrete Example

Let's work through a simple 3-layer network where x is just a single number:

**Layer 1:**
```
xâ‚ = xâ‚€ + F(xâ‚€)
   = 5 + 2
   = 7
```

**Layer 2:**
```
xâ‚‚ = xâ‚ + F(xâ‚)
   = 7 + 1
   = 8
```

**Layer 3:**
```
xâ‚ƒ = xâ‚‚ + F(xâ‚‚)
   = 8 + (-1)
   = 7
```

Notice how:
- The original value (5) is preserved in every output: 7 = 5+2, 8 = 5+2+1, 7 = 5+2+1-1
- Each layer only learned small adjustments: +2, +1, -1
- We can trace back to the original input easily

### The Identity Mapping Property

The term **x_l** appears directly in **x_L** without any modification. This is called the "identity mapping" propertyâ€”the original signal passes through unchanged, like a highway running parallel to local roads.

This simple addition of **"+ x_l"** is why we can now train massive models with hundreds of layers. But to understand why this is so powerful, we need to dive into gradients.

---

## Part 2: Understanding Gradients - How Neural Networks Learn {#gradients}

### What is a Gradient?

A gradient is simply **how much the output changes when you slightly change the input**. That's it!

#### Real-World Analogy: Hiking

Imagine you're hiking on a hill:
- **Gradient** = the steepness and direction of the slope
- Large gradient â†’ steep hill (small step = big height change)
- Small gradient â†’ gentle slope (small step = tiny height change)
- Zero gradient â†’ flat ground (stepping doesn't change height)

In mathematical notation:
```
âˆ‚y/âˆ‚x = "how much does y change when we change x by a tiny bit?"
```

#### Simple Example

If `y = 2x`:
- When x = 3, then y = 6
- When x = 3.1, then y = 6.2

The gradient is: `âˆ‚y/âˆ‚x = 2`

This means: "for every 1 unit increase in x, y increases by 2 units."

### How Neural Networks Learn

Neural networks learn through a three-step process:

1. **Make a prediction** (forward pass)
2. **Calculate how wrong the prediction was** (compute the loss)
3. **Adjust the weights to reduce this loss** (backward pass using gradients)

But how do we know which direction to adjust the weights? **We use gradients!**

The gradient tells us:
- Which weights to change
- By how much
- In which direction (increase or decrease)

### Backpropagation: The Chain Rule in Action

Let me walk you through a complete example to show how gradients flow backward through a network.

#### Example Network

```
Input (x) â†’ Layer 1 â†’ Layer 2 â†’ Output (y) â†’ Loss (L)
```

More specifically:
```
x â†’ [+3] â†’ a â†’ [Ã—2] â†’ y â†’ [compare to target] â†’ L
```

Let's say:
- x = 1
- Layer 1 adds 3: `a = x + 3 = 4`
- Layer 2 multiplies by 2: `y = 2a = 8`
- Target is 10, so loss: `L = (y - 10)Â² = (8 - 10)Â² = 4`

#### Forward Pass (Making Predictions)

This is straightforward:
```
x = 1 â†’ a = 4 â†’ y = 8 â†’ L = 4
```

#### Backward Pass (Learning)

Now we want to know: **"How should we change x to reduce the loss L?"**

We need to calculate: `âˆ‚L/âˆ‚x` (how loss changes with respect to x)

**The Chain Rule:**
```
âˆ‚L/âˆ‚x = (âˆ‚L/âˆ‚y) Ã— (âˆ‚y/âˆ‚a) Ã— (âˆ‚a/âˆ‚x)
```

Think of it like: "the effect of x on L flows through a and y"

Let's calculate each part:

**Step 1: How does L change with y?**
```
L = (y - 10)Â²
âˆ‚L/âˆ‚y = 2(y - 10) = 2(8 - 10) = -4
```
*Interpretation: If we increase y by 1, L decreases by 4.*

**Step 2: How does y change with a?**
```
y = 2a
âˆ‚y/âˆ‚a = 2
```
*Interpretation: If we increase a by 1, y increases by 2.*

**Step 3: How does a change with x?**
```
a = x + 3
âˆ‚a/âˆ‚x = 1
```
*Interpretation: If we increase x by 1, a increases by 1.*

**Step 4: Multiply them together (Chain Rule)**
```
âˆ‚L/âˆ‚x = (-4) Ã— (2) Ã— (1) = -8
```

**What this means:** If we increase x by 0.1, the loss will decrease by about 0.8. So we should increase x!

### The Vanishing Gradient Problem

Now let's see what happens with many layersâ€”and why it's a disaster.

#### Example: Deep Network Without Residual Connections

```
x â†’ [Ã—0.5] â†’ [Ã—0.5] â†’ [Ã—0.5] â†’ [Ã—0.5] â†’ [Ã—0.5] â†’ y
```

Each layer multiplies by 0.5.

**Forward pass:**
```
x = 1 â†’ 0.5 â†’ 0.25 â†’ 0.125 â†’ 0.0625 â†’ 0.03125 = y
```

**Backward pass (calculating gradients):**

At each layer, `âˆ‚output/âˆ‚input = 0.5`

By the chain rule:
```
âˆ‚y/âˆ‚x = 0.5 Ã— 0.5 Ã— 0.5 Ã— 0.5 Ã— 0.5 = 0.03125
```

**The Problem:** The gradient becomes very small! If we have 50 layers:
```
0.5^50 â‰ˆ 0.00000000000001
```

This is **vanishing gradient**â€”the gradient becomes so tiny that:
1. Early layers barely learn anything
2. Training becomes extremely slow
3. The network can't effectively learn

#### Why It's Catastrophic

Remember: gradients tell us how to adjust weights. If the gradient is 0.0000001:
- We need to make MASSIVE changes to early layers to have any effect
- But we can only make small, careful updates
- So learning essentially stops

This is why deeper networks performed worseâ€”they couldn't learn!

### How Residual Connections Fix Everything

Now let's see the magic of residual connections.

#### The Mathematics

Recall that with residual connections:
```
x_{l+1} = x_l + F(x_l, W_l)
```

When we stack multiple layers (from layer l to layer L):
```
x_L = x_l + Î£ F(x_i, W_i)
```

#### Taking the Gradient

Let's calculate how x_L changes with respect to x_l:

```
âˆ‚x_L/âˆ‚x_l = âˆ‚/âˆ‚x_l [x_l + Î£ F(x_i, W_i)]
```

Using the sum rule of derivatives:
```
âˆ‚x_L/âˆ‚x_l = âˆ‚x_l/âˆ‚x_l + âˆ‚/âˆ‚x_l[Î£ F(x_i, W_i)]
```

Since the derivative of something with respect to itself is always 1:

```
âˆ‚x_L/âˆ‚x_l = 1 + âˆ‚/âˆ‚x_l[Î£ F(x_i, W_i)]
```

### Why This "1" is Magical

Let's break down what this equation means:

```
âˆ‚x_L/âˆ‚x_l = 1 + [other stuff]
           â†‘     â†‘
      identity   residual
       path      path
```

**The identity path (the "1"):**
- Always present
- Always equals 1
- Never vanishes
- Provides a **guaranteed highway** for gradients to flow back

**The residual path (the "other stuff"):**
- Can be small
- Can even be zero
- But it doesn't matter because we still have the "1"!

#### Concrete Comparison

Let's say we have 5 layers, and the gradient through each residual function is small (0.1):

**Without residual connections:**
```
âˆ‚x_5/âˆ‚x_0 = 0.1 Ã— 0.1 Ã— 0.1 Ã— 0.1 Ã— 0.1 = 0.00001
```
Vanishing gradient! ðŸ˜¢

**With residual connections:**
```
âˆ‚x_5/âˆ‚x_0 = 1 + (something small)
          â‰ˆ 1 + 0.1
          = 1.1
```
The gradient stays healthy! ðŸ˜Š

Even if the residual path contributes almost nothing:
```
âˆ‚x_5/âˆ‚x_0 = 1 + 0.00001 â‰ˆ 1
```

The gradient is still 1, which is perfect for learning!

### Visual Comparison

**Traditional network (signal fades):**
```
Layer 1: gradient = 1.0
Layer 2: gradient = 0.5
Layer 3: gradient = 0.25
Layer 4: gradient = 0.125
Layer 5: gradient = 0.0625  â† Too small to learn!
```

**Residual network (signal preserved):**
```
Layer 1: gradient = 1.0 + small
Layer 2: gradient = 1.0 + small
Layer 3: gradient = 1.0 + small
Layer 4: gradient = 1.0 + small
Layer 5: gradient = 1.0 + small  â† Still strong!
```

### Key Takeaways

1. **Gradient = sensitivity**: How much output changes when input changes
2. **Vanishing gradient**: In deep networks, gradients multiply and become tiny
3. **Backpropagation**: Gradients flow backward using the chain rule
4. **Residual connections add "1"**: This ensures gradients always have a clear path
5. **The magic equation**: `âˆ‚x_L/âˆ‚x_l = 1 + ...` â€” the "1" is the gradient's superhighway

This is why residual connections revolutionized deep learning. They provide **guaranteed gradient flow** that never vanishes, allowing us to train very deep networks successfully!

---

## Part 3: The DeepSeek Innovation - Manifold-Constrained Hyper-Connections {#deepseek}

Now that we understand residual connections and gradients, we can appreciate DeepSeek's innovation and why it matters.

### The Evolution: From ResNets to Hyper-Connections

Residual connections have been the backbone of deep learning for nearly a decade. They work beautifully, but researchers keep asking: "Can we do better?"

Recently, a new approach called **Hyper-Connections (HC)** emerged, and it showed promising results. Think of it as an upgrade:

**Original Residual Connection:**
- One highway lane of information

**Hyper-Connections:**
- Multiple parallel highway lanes (say, 4 lanes instead of 1)

#### The Mathematics of Hyper-Connections

Instead of the simple residual formula:
```
x_{l+1} = x_l + F(x_l, W_l)
```

Hyper-Connections use:
```
x_{l+1} = H^res_l Â· x_l + H^post_l^T Â· F(H^pre_l Â· x_l, W_l)
```

Breaking this down:
- **x_l** is now expanded from dimension C to nÃ—C (multiple streams)
- **H^res_l**: A learnable mixing matrix for the residual stream
- **H^pre_l**: Aggregates features from multiple streams into the layer input
- **H^post_l**: Maps the layer output back onto multiple streams

#### The Visual Picture

Imagine instead of one conveyor belt, you have four parallel conveyor belts:

```
Stream 1: â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
Stream 2: â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
Stream 3: â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
Stream 4: â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•>
```

At each layer, information can:
1. Mix between streams (H^res)
2. Combine for processing (H^pre)
3. Distribute back to streams (H^post)

This gives the network more flexibility and capacity without significantly increasing computation (FLOPs remain similar).

### The Problem: Broken Identity Mapping

Here's where things get tricky. Remember our magical "1" from residual connections that guaranteed gradient flow? Hyper-Connections break it.

#### Multiple Layers with Hyper-Connections

When you stack multiple layers:

```
x_L = (âˆ H^res_{L-i}) Â· x_l + Î£ [(âˆ H^res_{L-j}) Â· H^post_i^T Â· F(...)]
```

Compare this to standard residual connections:
```
x_L = x_l + Î£ F(...)
      â†‘
  This "x_l" is the identityâ€”signals pass through unchanged
```

**The critical difference:** In Hyper-Connections, the term `(âˆ H^res_{L-i})` replaces the pure identity.

#### What Goes Wrong

The product of matrices `(âˆ H^res_{L-i})` can:
1. **Amplify signals**: Make them grow exponentially
2. **Attenuate signals**: Make them shrink toward zero
3. **Lose the conservation property**: The average signal strength across streams is not preserved

Think of it like this:
- **Standard ResNet**: You start with a signal of strength 5, and after 100 layers, it's still around 5
- **Hyper-Connections**: You start with 5, and after 100 layers it might be 500 or 0.005

This is disastrous for training stability, especially at large scale. It's like the vanishing gradient problem all over again, but now you can also have exploding gradients!

#### Why This Matters

When training very large models (billions of parameters, thousands of GPUs):
- Unstable training can waste millions of dollars in compute
- Exploding gradients can cause the model to diverge (produce nonsense)
- Vanishing gradients can cause learning to stall
- You can't reliably predict if training will succeed

### DeepSeek's Solution: Manifold-Constrained Hyper-Connections (mHC)

DeepSeek's key insight is brilliant: **constrain the mixing matrices to a special mathematical space where they preserve signal properties.**

#### The Mathematical Constraint

They use something called **doubly stochastic matrices** (matrices in the Birkhoff polytope). These are matrices where:
- Every row sums to 1
- Every column sums to 1

For example:
```
[0.3  0.7]     [0.25  0.25  0.25  0.25]
[0.7  0.3]     [0.2   0.3   0.3   0.2 ]
               [0.3   0.2   0.2   0.3 ]
               [0.25  0.25  0.25  0.25]
```

#### Why This Fixes Everything

When H^res is doubly stochastic:

1. **Signal conservation**: The operation `H^res Â· x` is a weighted average of inputs
   - No amplification (signals can't grow)
   - No attenuation (signals can't shrink)
   - Average strength is preserved

2. **Closure property**: The product of doubly stochastic matrices is also doubly stochastic
   - So `(âˆ H^res_{L-i})` maintains the same properties
   - This works no matter how many layers you stack!

3. **Gradient stability**: Since signals don't explode or vanish, neither do gradients

#### The Traffic Controller Analogy

Think of H^res as a traffic controller managing four highway lanes:

**Without constraints (original HC):**
- The controller could direct all traffic to one lane (signal amplification)
- Or spread it so thin that no lane gets enough (signal attenuation)
- After many intersections, traffic distribution becomes chaotic

**With doubly stochastic constraints (mHC):**
- The controller must keep total traffic constant
- Each lane gets a proper share
- After many intersections, traffic remains balanced

### How They Enforce the Constraint

DeepSeek uses the **Sinkhorn-Knopp algorithm**, which is an iterative method that projects any matrix onto the space of doubly stochastic matrices.

Think of it like this:
1. Start with any mixing matrix H^res
2. Repeatedly normalize rows and columns
3. After a few iterations, you get a doubly stochastic matrix
4. This happens automatically during training

The beauty is that this is differentiable, so gradients can still flow through it!

### Additional Optimizations

DeepSeek didn't stop at mathematical elegance. They also addressed practical concerns:

**1. Memory Efficiency**
- Multiple streams (nÃ—C dimensions) use more memory
- They use selective recomputing and kernel fusion
- Carefully overlap communication in distributed training

**2. Hardware Optimization**
- Developed custom kernels using TileLang
- Mixed precision computation
- Result: Only 6.7% additional time overhead when using 4 streams

### The Complete Picture

Let's visualize the full architecture:

**Standard ResNet:**
```
Input â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€> Layer â”€â”€â”€â”€â”¬â”€â”€â”€â”€> Output
           â”‚                  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           (identity, gradient = 1)
```

**Hyper-Connections (HC):**
```
Stream 1 â”€â”€â”¬â”€â”€> mix â”€â”€> Layer â”€â”€> mix â”€â”€â”¬â”€â”€> 
Stream 2 â”€â”€â”¤                              â”œâ”€â”€>
Stream 3 â”€â”€â”¤                              â”œâ”€â”€>
Stream 4 â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           (unconstrained mixing, unstable)
```

**Manifold-Constrained HC (mHC):**
```
Stream 1 â”€â”€â”¬â”€â”€> [constrained mix] â”€â”€> Layer â”€â”€> [constrained mix] â”€â”€â”¬â”€â”€> 
Stream 2 â”€â”€â”¤         (doubly                         (doubly          â”œâ”€â”€>
Stream 3 â”€â”€â”¤         stochastic)                     stochastic)      â”œâ”€â”€>
Stream 4 â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           (preserves signal strength and gradient flow)
```

### Why This Matters

1. **Best of both worlds**: Multiple streams (capacity) + stable training (identity mapping)
2. **Scalability**: Works reliably at large scale (billions of parameters)
3. **Practical efficiency**: Only 6.7% overhead with 4Ã— the representational capacity
4. **Theoretical grounding**: Mathematical guarantees about stability

### Experimental Results

DeepSeek tested mHC on large language model pretraining and found:
- Maintains the performance advantages of Hyper-Connections
- No training instabilities, even at scale
- Successfully trains models with billions of parameters
- Gradient flow remains healthy throughout training

### The Broader Impact

This work represents an important principle in deep learning research:

**Mathematical constraints can enable practical innovations.**

By understanding the fundamental properties that make residual connections work (identity mapping, gradient flow), and applying rigorous mathematical constraints (doubly stochastic matrices), DeepSeek has created an architecture that:
- Extends beyond simple residual connections
- Maintains the stability that made ResNets successful
- Opens new possibilities for even deeper, more capable models

---

## Conclusion: The Journey from ResNets to mHC

Let's trace the complete arc of this story:

**2016 - ResNets:** 
- Problem: Deep networks couldn't train
- Solution: Add identity mappings (skip connections)
- Result: Stable gradients, deep networks work

**2024 - Hyper-Connections:**
- Idea: Expand to multiple parallel streams
- Benefit: More capacity and flexibility
- Problem: Breaks identity mapping, causes instability

**2025 - DeepSeek's mHC:**
- Innovation: Constrain to doubly stochastic matrices
- Result: Multiple streams + stable identity mapping
- Impact: Scalable, efficient, theoretically grounded

The "1" in the equation `âˆ‚x_L/âˆ‚x_l = 1 + ...` was the key to deep learning's success. DeepSeek's contribution is showing how to preserve that essential property while pushing the boundaries of what neural network architectures can do.

This is deep learning research at its best: taking something that works (residual connections), understanding deeply why it works (identity mapping and gradient flow), and carefully extending it with mathematical rigor to create something better.

---

## Further Reading

If you want to dive deeper:
- **ResNets**: "Deep Residual Learning for Image Recognition" (He et al., 2016)
- **Gradient Flow**: "Understanding the difficulty of training deep feedforward neural networks" (Glorot & Bengio, 2010)
- **Hyper-Connections**: The original HC paper (Zhu et al., 2024)
- **DeepSeek mHC**: "Manifold-Constrained Hyper-Connections" (DeepSeek, 2025)

---

*This blog post was written to make complex deep learning concepts accessible to everyone. If you found it helpful, consider sharing it with others who are learning about neural networks!*
